{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c436508",
      "metadata": {
        "id": "2c436508"
      },
      "source": [
        "### Recognizing named entities off tweets using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Idea of Named Entity Recognition (NER) is to extract \"named entities\" from a text corpus. Examples of these entities may be person names, location, calendar months, course ID specific to a department etc. <br>\n",
        "So if you were to extract named entities - *person (PER)*, *course ID (C_ID)* of the sentence below: <br>\n",
        "**L645 is taught by Francis Tyers** <br>\n",
        "The NER model that you're going to be building would output a sequence of tags associated with the sentence as shown below: <br>\n",
        "**B-C_ID    O    O    O    B-PER    I-PER**\n",
        "\n",
        "THE B, I and O that you see above represent a prefix scheme known as *BIO markup* The B represents Beginning, I represents Inside and O represents Outside/Out-of. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-S1keQ7Cc_9c"
      },
      "id": "-S1keQ7Cc_9c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will use a recurrent neural network (RNN) model to carry out the aforementioned NER task. <br>\n",
        "The notebook has been broken down into 15 different sections that you will run sequentially to complete this exercise. <br> \n",
        "There are few lines of code that you're expected to complete in those different sections to keep you engaged. You'll find them labeled as either **COMPLETE** or **YOUR CODE HERE** in their respective places.\n"
      ],
      "metadata": {
        "id": "d_t23qmb0SEe"
      },
      "id": "d_t23qmb0SEe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recommend you use [Google Collab](https://colab.research.google.com/) for this exercise. <br>\n",
        "To get started, you're going to want to run the below the cell to download the data corpus you'd be using in the exercise."
      ],
      "metadata": {
        "id": "V5C_6PyC1v85"
      },
      "id": "V5C_6PyC1v85"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/train.txt # train data\n",
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/validation.txt # validation data\n",
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/data/test.txt # test data\n",
        "!wget https://raw.githubusercontent.com/heisenberg967/ner_twitter/main/evaluation.py # used in evaluation function to calculate f-score"
      ],
      "metadata": {
        "id": "01pnSiCG2LFF"
      },
      "id": "01pnSiCG2LFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "492bd41a",
      "metadata": {
        "id": "492bd41a"
      },
      "source": [
        "#### 1. Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you're going to load the dataset and invoke the read_data function. Within the read_data function, you're going to replace all occurences of '@' with the token \\<USR\\>. You may notice how URLs in tweets are replaced with the token \\<URL\\> for your reference."
      ],
      "metadata": {
        "id": "XiP6dZk7gslM"
      },
      "id": "XiP6dZk7gslM"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1115d91d",
      "metadata": {
        "id": "1115d91d"
      },
      "outputs": [],
      "source": [
        "### Function to read data\n",
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            \n",
        "            if token.lower().startswith('https://') or token.lower().startswith('http://'):\n",
        "                token = '<URL>'\n",
        "            # Replace all username char, i.e, '@' with <USR> token\n",
        "            ######### YOUR CODE HERE #############\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "df705d9f",
      "metadata": {
        "id": "df705d9f"
      },
      "outputs": [],
      "source": [
        "train_tokens, train_tags = read_data('train.txt')\n",
        "validation_tokens, validation_tags = read_data('validation.txt')\n",
        "test_tokens, test_tags = read_data('test.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to see what your data looks like. Feel free to play around with what you're printing."
      ],
      "metadata": {
        "id": "4wN1GQgViEzX"
      },
      "id": "4wN1GQgViEzX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07224733",
      "metadata": {
        "id": "07224733"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "488a293f",
      "metadata": {
        "id": "488a293f"
      },
      "source": [
        "#### 2. Create dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you're going to be generating two dictionaries or mappings: <br>\n",
        "\\{token\\} -> \\{tokenID\\} represents row in the embedding matrix for a token <br>\n",
        "\\{tag\\} -> \\{tagID\\} one-hot encoded vector to help compute loss at the output of the network <br>"
      ],
      "metadata": {
        "id": "wTsWCXA9i3L1"
      },
      "id": "wTsWCXA9i3L1"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    voc = set([x for ele in tokens_or_tags for x in ele])\n",
        "    voc_size = len(voc)+len(special_tokens)\n",
        "    idx2tok = ['']*voc_size\n",
        "    \n",
        "    # Create mappings from tokens to indices and vice versa\n",
        "    # Add special tokens to dictionaries\n",
        "    # The first special token must have index 0\n",
        "    ######################################\n",
        "    ######### YOUR CODE HERE #############\n",
        "    ######################################\n",
        "    \n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n",
        "\n",
        "# functions will help you to create the mapping between tokens and ids for a sentence\n",
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "metadata": {
        "id": "EnCXR3WZd-mt"
      },
      "id": "EnCXR3WZd-mt",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Generate Batches"
      ],
      "metadata": {
        "id": "FkHEO6KBS_ae"
      },
      "id": "FkHEO6KBS_ae"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below has been created to help train our model in batches. Now, since we want all sequences within a batch to have the same length, we're going to be padding a token *\\<PAD\\>*, as you may notice in the function."
      ],
      "metadata": {
        "id": "_i9Mj2dflOTX"
      },
      "id": "_i9Mj2dflOTX"
    },
    {
      "cell_type": "code",
      "source": [
        "# generate batches\n",
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>'] # pad token to ensure equal length\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "metadata": {
        "id": "nOghuFnMeTsy"
      },
      "id": "nOghuFnMeTsy",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Define empty LSTM class"
      ],
      "metadata": {
        "id": "nJViDMJBTFO2"
      },
      "id": "nJViDMJBTFO2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To carry out our NER task, we're going to be building an LSTM model whose purpose is to output a probability distribution over tags for each token in a sentence. Since, we're concerned with both left and right contexts of the token, we're using a bi-directional LSTM. We're also using a dense layer to perform tag classification."
      ],
      "metadata": {
        "id": "Gizcp2ltmvqc"
      },
      "id": "Gizcp2ltmvqc"
    },
    {
      "cell_type": "code",
      "source": [
        "### Import tf and Init LSTM Class \n",
        "\n",
        "# We're using version1 of tensorflow for this exercise.\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "class BiLSTMModel():\n",
        "    pass"
      ],
      "metadata": {
        "id": "wcZzHSefh2Eh"
      },
      "id": "wcZzHSefh2Eh",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Define placeholders for network model"
      ],
      "metadata": {
        "id": "6Tgq8m0jTJuo"
      },
      "id": "6Tgq8m0jTJuo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're first going to declare the following [placeholders](https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder) to specify what data is going into the network.\n",
        " - *input_batch* — sequences of words (the shape equals to [batch_size, sequence_len]);\n",
        " - *ground_truth_tags* — sequences of tags (the shape equals to [batch_size, sequence_len]);\n",
        " - *lengths* — lengths of not padded sequences (the shape equals to [batch_size]);\n",
        " - *dropout_ph* — dropout keep probability; this placeholder has a predefined value 1;\n",
        " - *learning_rate_ph* — learning rate; we need this placeholder because we want to change the value during training.\n",
        "\n",
        "Defining shape as None lets you feed in data of variable size."
      ],
      "metadata": {
        "id": "GNyj57cDoMfq"
      },
      "id": "GNyj57cDoMfq"
    },
    {
      "cell_type": "code",
      "source": [
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags')\n",
        "  \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)"
      ],
      "metadata": {
        "id": "dajnItQpoF-W"
      },
      "id": "dajnItQpoF-W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Define Layers of the network model."
      ],
      "metadata": {
        "id": "lSfDMKmBpBcG"
      },
      "id": "lSfDMKmBpBcG"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/Variable\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_value=, name='embedding_matrix', dtype=tf.float32) ######### COMPLETE the value for initial_value #############\n",
        "    \n",
        "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
        "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
        "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph)  ######### YOUR CODE HERE #############\n",
        "    backward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
        "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph) ######### YOUR CODE HERE #############\n",
        "\n",
        "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)\n",
        "    \n",
        "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
        "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, self.lengths, dtype=tf.float32) ######### YOUR CODE HERE #############\n",
        "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    # Dense layer on top.\n",
        "    # Shape: [batch_size, sequence_len, n_tags].   \n",
        "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
        "\n",
        "BiLSTMModel.__build_layers = classmethod(build_layers)"
      ],
      "metadata": {
        "id": "xHspjLArqf2_"
      },
      "id": "xHspjLArqf2_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Compute Predictions and Loss"
      ],
      "metadata": {
        "id": "JRR23hpXqjC2"
      },
      "id": "JRR23hpXqjC2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we're going to apply softmax to the last layer and use argmax to determine the most probable tags as defined in the function compute_predictions. <br>\n",
        "The compute_loss function is used to create our loss function for which we're making use of tensorflow's [cross entropy with logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2). Also, we're going to be ignoring loss coming from the \\<PAD\\> tokens we'd created earlier."
      ],
      "metadata": {
        "id": "T_gXlrpNr8RV"
      },
      "id": "T_gXlrpNr8RV"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/math/softmax\n",
        "    # Your task is to create a softmax function and assign it to softmax_output.\n",
        "    softmax_output = ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Use argmax (tf.argmax) to get the most probable tags with axis=-1\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/argmax\n",
        "    self.predictions = tf.argmax(softmax_output, axis=-1)\n",
        "\n",
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)\n",
        "\n",
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_tags_one_hot, logits=self.logits)\n",
        "    \n",
        "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "    mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
        "    self.loss =  tf.reduce_mean(tf.multiply(loss_tensor, mask))\n",
        "\n",
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)"
      ],
      "metadata": {
        "id": "uXxz7XTDtxsX"
      },
      "id": "uXxz7XTDtxsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Optimize loss using Adam optimizer"
      ],
      "metadata": {
        "id": "JrUKrBqRt0b5"
      },
      "id": "JrUKrBqRt0b5"
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    # Your task is to create an optimizer (tf.train.AdamOptimizer)\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer\n",
        "    self.optimizer =  ######### YOUR CODE HERE #############\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "    \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Pay attention that you need to apply this operation only for gradients \n",
        "    # because self.grads_and_vars contains also variables.\n",
        "    # list comprehension mught be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars =[ (tf.clip_by_norm(x[0], clip_norm),x[1]) for x in self.grads_and_vars]\n",
        "    \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        "\n",
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "metadata": {
        "id": "1HeIX1UAeXuU"
      },
      "id": "1HeIX1UAeXuU",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Build LSTM class"
      ],
      "metadata": {
        "id": "jI3Z8Ce3TNrf"
      },
      "id": "jI3Z8Ce3TNrf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! So, we've defined all the components in our network model, so we pass on the functions onto our LSTM class' constructor."
      ],
      "metadata": {
        "id": "_YNcrb6buYlE"
      },
      "id": "_YNcrb6buYlE"
    },
    {
      "cell_type": "code",
      "source": [
        "## Build LSTM class\n",
        "\n",
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()\n",
        "\n",
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ],
      "metadata": {
        "id": "mrStfaW7eXqJ"
      },
      "id": "mrStfaW7eXqJ",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Train Neural Network Model"
      ],
      "metadata": {
        "id": "JMrFzaZHTQck"
      },
      "id": "JMrFzaZHTQck"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train the network model that we've built, we're going to be computing *self.train_op* that we'd declared within the *perform_optimization* function. We're going to feed our actual data into the placeholders we'd defined as you may observe in *feed_dict* "
      ],
      "metadata": {
        "id": "ZCttOatKu8ro"
      },
      "id": "ZCttOatKu8ro"
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN NN\n",
        "\n",
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    # https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session\n",
        "    session.run(self.train_op, feed_dict=feed_dict)\n",
        "\n",
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "metadata": {
        "id": "AlDnOwFXeXiE"
      },
      "id": "AlDnOwFXeXiE",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11. Generate tag predictions"
      ],
      "metadata": {
        "id": "bjK3LNrgTTXi"
      },
      "id": "bjK3LNrgTTXi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict tags, we're going to compute self.predictions"
      ],
      "metadata": {
        "id": "Y37ITXpMwtm5"
      },
      "id": "Y37ITXpMwtm5"
    },
    {
      "cell_type": "code",
      "source": [
        "## PREDICT\n",
        "\n",
        "def predict_for_batch(self, session, x_batch, lengths):    \n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.dropout_ph: 1.0,\n",
        "                 self.lengths: lengths}\n",
        "\n",
        "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
      ],
      "metadata": {
        "id": "sYTzxfa7eXQY"
      },
      "id": "sYTzxfa7eXQY",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12. Evaluate Model"
      ],
      "metadata": {
        "id": "67XcagI9TbRz"
      },
      "id": "67XcagI9TbRz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To help in evaluating our model, we're going to create two functions. <br>\n",
        "The *predict_tags* gets predictions from a network model and then transforms indices to tokens and tags. <br>\n",
        "The *eval_conll* function calculates precision, recall and F1 score. <br>\n"
      ],
      "metadata": {
        "id": "a4WaxZHNxEg5"
      },
      "id": "a4WaxZHNxEg5"
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate model \n",
        "\n",
        "from evaluation import precision_recall_f1\n",
        "\n",
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "metadata": {
        "id": "r2EJPs7keUNf"
      },
      "id": "r2EJPs7keUNf",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13. Set/Adjust hyperparameters for BiLSTM"
      ],
      "metadata": {
        "id": "wkmkcX8SThQ3"
      },
      "id": "wkmkcX8SThQ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in this section, you're going to see all the different pieces come together. <br>\n",
        "\n",
        "Our *BiLSTMModel* model has the following parameters:\n",
        " - *vocabulary_size* — number of tokens;\n",
        " - *n_tags* — number of tags;\n",
        " - *embedding_dim* — dimension of embeddings, recommended value: 200;\n",
        " - *n_hidden_rnn* — size of hidden layers for RNN, recommended value: 200;\n",
        " - *PAD_index* — an index of the padding token (\\<PAD\\>).\n",
        "\n",
        "Run your model by setting different hyperparameters. You could start with the following values\n",
        "- *batch_size*: 32;\n",
        "- 4 epochs;\n",
        "- starting value of *learning_rate*: 0.005\n",
        "- *learning_rate_decay*: a square root of 2;\n",
        "- *dropout_keep_probability*: try several values: 0.1, 0.5, 0.9.\n",
        "\n"
      ],
      "metadata": {
        "id": "SwP7PAmKx2_j"
      },
      "id": "SwP7PAmKx2_j"
    },
    {
      "cell_type": "code",
      "source": [
        "### RUN NN by setting different hyperparameters\n",
        "### BUILD the model\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])\n",
        "\n",
        "batch_size = ######### YOUR CODE HERE #############\n",
        "n_epochs = ######### YOUR CODE HERE #############\n",
        "learning_rate = ######### YOUR CODE HERE #############\n",
        "learning_rate_decay = ######### YOUR CODE HERE #############\n",
        "dropout_keep_probability = ######### YOUR CODE HERE #############"
      ],
      "metadata": {
        "id": "7Uckl7Fofvwo"
      },
      "id": "7Uckl7Fofvwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 14. Run your model with the train/validation data"
      ],
      "metadata": {
        "id": "LeBzw5A0Tr3Q"
      },
      "id": "LeBzw5A0Tr3Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay! We're now ready to run our model!"
      ],
      "metadata": {
        "id": "aEm6xfyTy_GW"
      },
      "id": "aEm6xfyTy_GW"
    },
    {
      "cell_type": "code",
      "source": [
        "### Train model using dataset\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "metadata": {
        "id": "hpcXm_FZfvuC"
      },
      "id": "hpcXm_FZfvuC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 15. Look at Results"
      ],
      "metadata": {
        "id": "-gPPuEOXTwWw"
      },
      "id": "-gPPuEOXTwWw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats on making it till the end! You may observe your results by running the cell below. If you did things right, the F1 score on your validation set should be close to 40%."
      ],
      "metadata": {
        "id": "-AQoCTXmz5NZ"
      },
      "id": "-AQoCTXmz5NZ"
    },
    {
      "cell_type": "code",
      "source": [
        "## RESULT\n",
        "\n",
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False) ######### YOUR CODE HERE #############\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False) ######### YOUR CODE HERE #############"
      ],
      "metadata": {
        "id": "QfxH51CEfvrK"
      },
      "id": "QfxH51CEfvrK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2c436508",
        "492bd41a",
        "488a293f",
        "FkHEO6KBS_ae",
        "nJViDMJBTFO2",
        "6Tgq8m0jTJuo",
        "lSfDMKmBpBcG",
        "JRR23hpXqjC2",
        "JrUKrBqRt0b5",
        "jI3Z8Ce3TNrf",
        "JMrFzaZHTQck",
        "bjK3LNrgTTXi",
        "67XcagI9TbRz",
        "wkmkcX8SThQ3",
        "LeBzw5A0Tr3Q",
        "-gPPuEOXTwWw"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}