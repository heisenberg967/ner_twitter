{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c436508",
      "metadata": {
        "id": "2c436508"
      },
      "source": [
        "### Recognizing named entities off tweets using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492bd41a",
      "metadata": {
        "id": "492bd41a"
      },
      "source": [
        "#### 1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1115d91d",
      "metadata": {
        "id": "1115d91d"
      },
      "outputs": [],
      "source": [
        "### Function to read data\n",
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    #Ex: \n",
        "    # token -\n",
        "    # tag - \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            \n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "\n",
        "            ######################################\n",
        "            ######### YOUR CODE HERE #############\n",
        "            ######################################\n",
        "            if token.startswith('@'):\n",
        "                token = '<USR>'\n",
        "            elif token.lower().startswith('https://') or token.lower().startswith('http://'):\n",
        "                token = '<URL>'\n",
        "            \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "df705d9f",
      "metadata": {
        "id": "df705d9f"
      },
      "outputs": [],
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "07224733",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07224733",
        "outputId": "1c134883-0cf4-4136-c72b-a3c8131e3bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT\tO\n",
            "<USR>\tO\n",
            ":\tO\n",
            "Online\tO\n",
            "ticket\tO\n",
            "sales\tO\n",
            "for\tO\n",
            "Ghostland\tB-musicartist\n",
            "Observatory\tI-musicartist\n",
            "extended\tO\n",
            "until\tO\n",
            "6\tO\n",
            "PM\tO\n",
            "EST\tO\n",
            "due\tO\n",
            "to\tO\n",
            "high\tO\n",
            "demand\tO\n",
            ".\tO\n",
            "Get\tO\n",
            "them\tO\n",
            "before\tO\n",
            "they\tO\n",
            "sell\tO\n",
            "out\tO\n",
            "...\tO\n",
            "\n",
            "Apple\tB-product\n",
            "MacBook\tI-product\n",
            "Pro\tI-product\n",
            "A1278\tI-product\n",
            "13.3\tI-product\n",
            "\"\tI-product\n",
            "Laptop\tI-product\n",
            "-\tI-product\n",
            "MD101LL/A\tI-product\n",
            "(\tO\n",
            "June\tO\n",
            ",\tO\n",
            "2012\tO\n",
            ")\tO\n",
            "-\tO\n",
            "Full\tO\n",
            "read\tO\n",
            "by\tO\n",
            "eBay\tB-company\n",
            "<URL>\tO\n",
            "<URL>\tO\n",
            "\n",
            "Happy\tO\n",
            "Birthday\tO\n",
            "<USR>\tO\n",
            "!\tO\n",
            "May\tO\n",
            "Allah\tB-person\n",
            "s.w.t\tO\n",
            "bless\tO\n",
            "you\tO\n",
            "with\tO\n",
            "goodness\tO\n",
            "and\tO\n",
            "happiness\tO\n",
            ".\tO\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "488a293f",
      "metadata": {
        "id": "488a293f"
      },
      "source": [
        "#### 2. Prepare dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens to indices and vice versa\n",
        "    # Add special tokens to dictionaries\n",
        "    # The first special token must have index 0\n",
        "    \n",
        "    ######################################\n",
        "    ######### YOUR CODE HERE #############\n",
        "    ######################################\n",
        "    voc = set([x for ele in tokens_or_tags for x in ele])\n",
        "    voc_size = len(voc)+len(special_tokens)\n",
        "    idx2tok = ['']*voc_size\n",
        "\n",
        "    for i,token in enumerate(special_tokens):\n",
        "        tok2idx[token] = i\n",
        "        idx2tok[i] = token\n",
        "    \n",
        "    for i, token in enumerate(voc, len(special_tokens)):\n",
        "        tok2idx[token] = i\n",
        "        idx2tok[i] = token\n",
        "        \n",
        "    \n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)\n",
        "\n",
        "# functions will help you to create the mapping between tokens and ids for a sentence\n",
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "metadata": {
        "id": "EnCXR3WZd-mt"
      },
      "id": "EnCXR3WZd-mt",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Generate Batches"
      ],
      "metadata": {
        "id": "FkHEO6KBS_ae"
      },
      "id": "FkHEO6KBS_ae"
    },
    {
      "cell_type": "code",
      "source": [
        "# generate batches\n",
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    \n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "metadata": {
        "id": "nOghuFnMeTsy"
      },
      "id": "nOghuFnMeTsy",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Define empty LSTM class"
      ],
      "metadata": {
        "id": "nJViDMJBTFO2"
      },
      "id": "nJViDMJBTFO2"
    },
    {
      "cell_type": "code",
      "source": [
        "### Import tf and Init LSTM Class \n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "class BiLSTMModel():\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcZzHSefh2Eh",
        "outputId": "a339baf1-cbbf-425b-8232-e6047e354273"
      },
      "id": "wcZzHSefh2Eh",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Define parameters for LSTM"
      ],
      "metadata": {
        "id": "6Tgq8m0jTJuo"
      },
      "id": "6Tgq8m0jTJuo"
    },
    {
      "cell_type": "code",
      "source": [
        "## Functions to define parameters for LSTM\n",
        "\n",
        "\n",
        "def declare_placeholders(self):\n",
        "    \"\"\"Specifies placeholders for the model.\"\"\"\n",
        "\n",
        "    # Placeholders for input and ground truth output.\n",
        "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None], name='input_batch') \n",
        "    self.ground_truth_tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_tags') ######### YOUR CODE HERE #############\n",
        "  \n",
        "    # Placeholder for lengths of the sequences.\n",
        "    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths') \n",
        "    \n",
        "    # Placeholder for a dropout keep probability. If we don't feed\n",
        "    # a value for this placeholder, it will be equal to 1.0.\n",
        "    self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
        "    \n",
        "    # Placeholder for a learning rate (tf.float32).\n",
        "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate') ######### YOUR CODE HERE #############\n",
        "\n",
        "BiLSTMModel.__declare_placeholders = classmethod(declare_placeholders)\n",
        "\n",
        "def build_layers(self, vocabulary_size, embedding_dim, n_hidden_rnn, n_tags):\n",
        "    \"\"\"Specifies bi-LSTM architecture and computes logits for inputs.\"\"\"\n",
        "    \n",
        "    # Create embedding variable (tf.Variable) with dtype tf.float32\n",
        "    initial_embedding_matrix = np.random.randn(vocabulary_size, embedding_dim) / np.sqrt(embedding_dim)\n",
        "    embedding_matrix_variable = tf.Variable(initial_value=initial_embedding_matrix, name='embedding_matrix', dtype=tf.float32) ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Create RNN cells (for example, tf.nn.rnn_cell.BasicLSTMCell) with n_hidden_rnn number of units \n",
        "    # and dropout (tf.nn.rnn_cell.DropoutWrapper), initializing all *_keep_prob with dropout placeholder.\n",
        "    forward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
        "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph)  ######### YOUR CODE HERE #############\n",
        "    backward_cell =  tf.nn.rnn_cell.DropoutWrapper(\n",
        "        tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden_rnn), input_keep_prob=self.dropout_ph, output_keep_prob=self.dropout_ph, state_keep_prob=self.dropout_ph) ######### YOUR CODE HERE #############\n",
        "\n",
        "    # Look up embeddings for self.input_batch (tf.nn.embedding_lookup).\n",
        "    # Shape: [batch_size, sequence_len, embedding_dim].\n",
        "    embeddings =  tf.nn.embedding_lookup(embedding_matrix_variable, self.input_batch)   ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Pass them through Bidirectional Dynamic RNN (tf.nn.bidirectional_dynamic_rnn).\n",
        "    # Shape: [batch_size, sequence_len, 2 * n_hidden_rnn]. \n",
        "    # Also don't forget to initialize sequence_length as self.lengths and dtype as tf.float32.\n",
        "    (rnn_output_fw, rnn_output_bw), _ =  tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, embeddings, self.lengths, dtype=tf.float32) ######### YOUR CODE HERE #############\n",
        "    rnn_output = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n",
        "\n",
        "    # Dense layer on top.\n",
        "    # Shape: [batch_size, sequence_len, n_tags].   \n",
        "    self.logits = tf.layers.dense(rnn_output, n_tags, activation=None)\n",
        "\n",
        "BiLSTMModel.__build_layers = classmethod(build_layers)\n",
        "\n",
        "def compute_predictions(self):\n",
        "    \"\"\"Transforms logits to probabilities and finds the most probable tags.\"\"\"\n",
        "    \n",
        "    # Create softmax (tf.nn.softmax) function\n",
        "    softmax_output = tf.nn.softmax(self.logits) ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Use argmax (tf.argmax) to get the most probable tags\n",
        "    # Don't forget to set axis=-1\n",
        "    # otherwise argmax will be calculated in a wrong way\n",
        "    self.predictions = tf.argmax(softmax_output, axis=-1) ######### YOUR CODE HERE #############\n",
        "\n",
        "BiLSTMModel.__compute_predictions = classmethod(compute_predictions)\n",
        "\n",
        "def compute_loss(self, n_tags, PAD_index):\n",
        "    \"\"\"Computes masked cross-entopy loss with logits.\"\"\"\n",
        "    \n",
        "    # Create cross entropy function function (tf.nn.softmax_cross_entropy_with_logits)\n",
        "    ground_truth_tags_one_hot = tf.one_hot(self.ground_truth_tags, n_tags)\n",
        "    loss_tensor =  tf.nn.softmax_cross_entropy_with_logits_v2(labels=ground_truth_tags_one_hot, logits=self.logits)  ######### YOUR CODE HERE #############\n",
        "    \n",
        "    # Create loss function which doesn't operate with <PAD> tokens (tf.reduce_mean)\n",
        "    mask = tf.cast(tf.not_equal(loss_tensor, PAD_index), tf.float32)\n",
        "    self.loss =  tf.reduce_mean(tf.multiply(loss_tensor, mask))  ######### YOUR CODE HERE #############\n",
        "\n",
        "BiLSTMModel.__compute_loss = classmethod(compute_loss)\n",
        "\n",
        "def perform_optimization(self):\n",
        "    \"\"\"Specifies the optimizer and train_op for the model.\"\"\"\n",
        "    \n",
        "    # Create an optimizer (tf.train.AdamOptimizer)\n",
        "    self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph) ######### YOUR CODE HERE #############\n",
        "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
        "    \n",
        "    # Gradient clipping (tf.clip_by_norm) for self.grads_and_vars\n",
        "    # Pay attention that you need to apply this operation only for gradients \n",
        "    # because self.grads_and_vars contains also variables.\n",
        "    # list comprehension mught be useful in this case.\n",
        "    clip_norm = tf.cast(1.0, tf.float32)\n",
        "    self.grads_and_vars =[ (tf.clip_by_norm(x[0], clip_norm),x[1]) for x in self.grads_and_vars]######### YOUR CODE HERE #############\n",
        "    \n",
        "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)\n",
        "\n",
        "BiLSTMModel.__perform_optimization = classmethod(perform_optimization)"
      ],
      "metadata": {
        "id": "1HeIX1UAeXuU"
      },
      "id": "1HeIX1UAeXuU",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Build LSTM class"
      ],
      "metadata": {
        "id": "jI3Z8Ce3TNrf"
      },
      "id": "jI3Z8Ce3TNrf"
    },
    {
      "cell_type": "code",
      "source": [
        "## Build LSTM class\n",
        "\n",
        "def init_model(self, vocabulary_size, n_tags, embedding_dim, n_hidden_rnn, PAD_index):\n",
        "    self.__declare_placeholders()\n",
        "    self.__build_layers(vocabulary_size, embedding_dim, n_hidden_rnn, n_tags)\n",
        "    self.__compute_predictions()\n",
        "    self.__compute_loss(n_tags, PAD_index)\n",
        "    self.__perform_optimization()\n",
        "\n",
        "BiLSTMModel.__init__ = classmethod(init_model)"
      ],
      "metadata": {
        "id": "mrStfaW7eXqJ"
      },
      "id": "mrStfaW7eXqJ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Train Neural Network Model"
      ],
      "metadata": {
        "id": "JMrFzaZHTQck"
      },
      "id": "JMrFzaZHTQck"
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAIN NN\n",
        "\n",
        "def train_on_batch(self, session, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability):\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.ground_truth_tags: y_batch,\n",
        "                 self.learning_rate_ph: learning_rate,\n",
        "                 self.dropout_ph: dropout_keep_probability,\n",
        "                 self.lengths: lengths}\n",
        "    \n",
        "    session.run(self.train_op, feed_dict=feed_dict)\n",
        "\n",
        "BiLSTMModel.train_on_batch = classmethod(train_on_batch)"
      ],
      "metadata": {
        "id": "AlDnOwFXeXiE"
      },
      "id": "AlDnOwFXeXiE",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Generate predictions from batches"
      ],
      "metadata": {
        "id": "bjK3LNrgTTXi"
      },
      "id": "bjK3LNrgTTXi"
    },
    {
      "cell_type": "code",
      "source": [
        "## PREDICT\n",
        "\n",
        "def predict_for_batch(self, session, x_batch, lengths):\n",
        "    ######################################\n",
        "    ######### YOUR CODE HERE #############\n",
        "    ######################################\n",
        "    feed_dict = {self.input_batch: x_batch,\n",
        "                 self.dropout_ph: 1.0,\n",
        "                 self.lengths: lengths}\n",
        "\n",
        "    predictions = session.run(self.predictions, feed_dict=feed_dict)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "BiLSTMModel.predict_for_batch = classmethod(predict_for_batch)"
      ],
      "metadata": {
        "id": "sYTzxfa7eXQY"
      },
      "id": "sYTzxfa7eXQY",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Evaluate Model"
      ],
      "metadata": {
        "id": "67XcagI9TbRz"
      },
      "id": "67XcagI9TbRz"
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluate model \n",
        "\n",
        "from evaluation import precision_recall_f1\n",
        "\n",
        "def predict_tags(model, session, token_idxs_batch, lengths):\n",
        "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
        "    \n",
        "    tag_idxs_batch = model.predict_for_batch(session, token_idxs_batch, lengths)\n",
        "    \n",
        "    tags_batch, tokens_batch = [], []\n",
        "    for tag_idxs, token_idxs in zip(tag_idxs_batch, token_idxs_batch):\n",
        "        tags, tokens = [], []\n",
        "        for tag_idx, token_idx in zip(tag_idxs, token_idxs):\n",
        "            tags.append(idx2tag[tag_idx])\n",
        "            tokens.append(idx2token[token_idx])\n",
        "        tags_batch.append(tags)\n",
        "        tokens_batch.append(tokens)\n",
        "    return tags_batch, tokens_batch\n",
        "    \n",
        "    \n",
        "def eval_conll(model, session, tokens, tags, short_report=True):\n",
        "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
        "    \n",
        "    y_true, y_pred = [], []\n",
        "    for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
        "        tags_batch, tokens_batch = predict_tags(model, session, x_batch, lengths)\n",
        "        if len(x_batch[0]) != len(tags_batch[0]):\n",
        "            raise Exception(\"Incorrect length of prediction for the input, \"\n",
        "                            \"expected length: %i, got: %i\" % (len(x_batch[0]), len(tags_batch[0])))\n",
        "        predicted_tags = []\n",
        "        ground_truth_tags = []\n",
        "        for gt_tag_idx, pred_tag, token in zip(y_batch[0], tags_batch[0], tokens_batch[0]): \n",
        "            if token != '<PAD>':\n",
        "                ground_truth_tags.append(idx2tag[gt_tag_idx])\n",
        "                predicted_tags.append(pred_tag)\n",
        "\n",
        "        # We extend every prediction and ground truth sequence with 'O' tag\n",
        "        # to indicate a possible end of entity.\n",
        "        y_true.extend(ground_truth_tags + ['O'])\n",
        "        y_pred.extend(predicted_tags + ['O'])\n",
        "        \n",
        "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
        "    return results"
      ],
      "metadata": {
        "id": "r2EJPs7keUNf"
      },
      "id": "r2EJPs7keUNf",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Invoke LSTM model defined earlier by finetuning hyperparameters"
      ],
      "metadata": {
        "id": "wkmkcX8SThQ3"
      },
      "id": "wkmkcX8SThQ3"
    },
    {
      "cell_type": "code",
      "source": [
        "### RUN NN by setting different hyperparameters\n",
        "### BUILD the model\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "model = BiLSTMModel(vocabulary_size=len(token2idx), n_tags=len(tag2idx), embedding_dim=200, n_hidden_rnn=200, PAD_index=token2idx['<PAD>'])  ######### YOUR CODE HERE #############\n",
        "\n",
        "batch_size = 32 ######### YOUR CODE HERE #############\n",
        "n_epochs = 8 ######### YOUR CODE HERE #############\n",
        "learning_rate = 0.02 ######### YOUR CODE HERE #############\n",
        "learning_rate_decay = np.sqrt(2) ######### YOUR CODE HERE #############\n",
        "dropout_keep_probability = 0.9 ######### YOUR CODE HERE #############"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Uckl7Fofvwo",
        "outputId": "31c628b9-9ed7-4451-93db-86e192fe5446"
      },
      "id": "7Uckl7Fofvwo",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-c19b542963b9>:44: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:446: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:756: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  shape=[input_depth + h_depth, 4 * self._num_units])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11. Train the model with the given train dataset."
      ],
      "metadata": {
        "id": "LeBzw5A0Tr3Q"
      },
      "id": "LeBzw5A0Tr3Q"
    },
    {
      "cell_type": "code",
      "source": [
        "### Train model using dataset\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Start training... \\n')\n",
        "for epoch in range(n_epochs):\n",
        "    # For each epoch evaluate the model on train and validation data\n",
        "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
        "    print('Train data evaluation:')\n",
        "    eval_conll(model, sess, train_tokens, train_tags, short_report=True)\n",
        "    print('Validation data evaluation:')\n",
        "    eval_conll(model, sess, validation_tokens, validation_tags, short_report=True)\n",
        "    \n",
        "    # Train the model\n",
        "    for x_batch, y_batch, lengths in batches_generator(batch_size, train_tokens, train_tags):\n",
        "        model.train_on_batch(sess, x_batch, y_batch, lengths, learning_rate, dropout_keep_probability)\n",
        "        \n",
        "    # Decaying the learning rate\n",
        "    learning_rate = learning_rate / learning_rate_decay\n",
        "    \n",
        "print('...training finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpcXm_FZfvuC",
        "outputId": "a0b4a8d3-a9f0-43d9-89bf-e5f799beb0d1"
      },
      "id": "hpcXm_FZfvuC",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training... \n",
            "\n",
            "-------------------- Epoch 1 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 73865 phrases; correct: 141.\n",
            "\n",
            "precision:  0.19%; recall:  3.14%; F1:  0.36\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 8927 phrases; correct: 16.\n",
            "\n",
            "precision:  0.18%; recall:  2.98%; F1:  0.34\n",
            "\n",
            "-------------------- Epoch 2 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 3403 phrases; correct: 1379.\n",
            "\n",
            "precision:  40.52%; recall:  30.72%; F1:  34.95\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 236 phrases; correct: 82.\n",
            "\n",
            "precision:  34.75%; recall:  15.27%; F1:  21.22\n",
            "\n",
            "-------------------- Epoch 3 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4661 phrases; correct: 2250.\n",
            "\n",
            "precision:  48.27%; recall:  50.12%; F1:  49.18\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 402 phrases; correct: 112.\n",
            "\n",
            "precision:  27.86%; recall:  20.86%; F1:  23.86\n",
            "\n",
            "-------------------- Epoch 4 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4345 phrases; correct: 3022.\n",
            "\n",
            "precision:  69.55%; recall:  67.32%; F1:  68.42\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 352 phrases; correct: 115.\n",
            "\n",
            "precision:  32.67%; recall:  21.42%; F1:  25.87\n",
            "\n",
            "-------------------- Epoch 5 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4742 phrases; correct: 3977.\n",
            "\n",
            "precision:  83.87%; recall:  88.59%; F1:  86.17\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 436 phrases; correct: 173.\n",
            "\n",
            "precision:  39.68%; recall:  32.22%; F1:  35.56\n",
            "\n",
            "-------------------- Epoch 6 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4662 phrases; correct: 3606.\n",
            "\n",
            "precision:  77.35%; recall:  80.33%; F1:  78.81\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 383 phrases; correct: 136.\n",
            "\n",
            "precision:  35.51%; recall:  25.33%; F1:  29.57\n",
            "\n",
            "-------------------- Epoch 7 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4209 phrases; correct: 3734.\n",
            "\n",
            "precision:  88.71%; recall:  83.18%; F1:  85.86\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 382 phrases; correct: 137.\n",
            "\n",
            "precision:  35.86%; recall:  25.51%; F1:  29.82\n",
            "\n",
            "-------------------- Epoch 8 of 8 --------------------\n",
            "Train data evaluation:\n",
            "processed 105778 tokens with 4489 phrases; found: 4139 phrases; correct: 3761.\n",
            "\n",
            "precision:  90.87%; recall:  83.78%; F1:  87.18\n",
            "\n",
            "Validation data evaluation:\n",
            "processed 12836 tokens with 537 phrases; found: 376 phrases; correct: 134.\n",
            "\n",
            "precision:  35.64%; recall:  24.95%; F1:  29.35\n",
            "\n",
            "...training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12. Look at Results"
      ],
      "metadata": {
        "id": "-gPPuEOXTwWw"
      },
      "id": "-gPPuEOXTwWw"
    },
    {
      "cell_type": "code",
      "source": [
        "## RESULT\n",
        "\n",
        "\n",
        "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
        "train_results = eval_conll(model, sess, train_tokens, train_tags, short_report=False)\n",
        "\n",
        "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
        "validation_results = eval_conll(model, sess, validation_tokens, validation_tags, short_report=False) ######### YOUR CODE HERE #############\n",
        "\n",
        "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
        "test_results = eval_conll(model, sess, test_tokens, test_tags, short_report=False) ######### YOUR CODE HERE #############"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfxH51CEfvrK",
        "outputId": "310cc864-bb96-4675-e058-5de2bc9ebef1"
      },
      "id": "QfxH51CEfvrK",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Train set quality: --------------------\n",
            "processed 105778 tokens with 4489 phrases; found: 4172 phrases; correct: 3791.\n",
            "\n",
            "precision:  90.87%; recall:  84.45%; F1:  87.54\n",
            "\n",
            "\t     company: precision:    1.32%; recall:    0.31%; F1:    0.50; predicted:   151\n",
            "\n",
            "\t    facility: precision:   95.90%; recall:   96.82%; F1:   96.35; predicted:   317\n",
            "\n",
            "\t     geo-loc: precision:   98.90%; recall:   99.70%; F1:   99.30; predicted:  1004\n",
            "\n",
            "\t       movie: precision:   94.12%; recall:   94.12%; F1:   94.12; predicted:    68\n",
            "\n",
            "\t musicartist: precision:   98.28%; recall:   98.71%; F1:   98.49; predicted:   233\n",
            "\n",
            "\t       other: precision:   94.97%; recall:   97.23%; F1:   96.08; predicted:   775\n",
            "\n",
            "\t      person: precision:   98.88%; recall:   99.44%; F1:   99.16; predicted:   891\n",
            "\n",
            "\t     product: precision:   69.71%; recall:   98.43%; F1:   81.62; predicted:   449\n",
            "\n",
            "\t  sportsteam: precision:   96.83%; recall:   98.62%; F1:   97.72; predicted:   221\n",
            "\n",
            "\t      tvshow: precision:   87.30%; recall:   94.83%; F1:   90.91; predicted:    63\n",
            "\n",
            "-------------------- Validation set quality: --------------------\n",
            "processed 12836 tokens with 537 phrases; found: 404 phrases; correct: 140.\n",
            "\n",
            "precision:  34.65%; recall:  26.07%; F1:  29.76\n",
            "\n",
            "\t     company: precision:    3.33%; recall:    0.96%; F1:    1.49; predicted:    30\n",
            "\n",
            "\t    facility: precision:   50.00%; recall:   38.24%; F1:   43.33; predicted:    26\n",
            "\n",
            "\t     geo-loc: precision:   69.66%; recall:   54.87%; F1:   61.39; predicted:    89\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     8\n",
            "\n",
            "\t musicartist: precision:   33.33%; recall:   17.86%; F1:   23.26; predicted:    15\n",
            "\n",
            "\t       other: precision:   26.97%; recall:   29.63%; F1:   28.24; predicted:    89\n",
            "\n",
            "\t      person: precision:   33.75%; recall:   24.11%; F1:   28.12; predicted:    80\n",
            "\n",
            "\t     product: precision:    6.52%; recall:    8.82%; F1:    7.50; predicted:    46\n",
            "\n",
            "\t  sportsteam: precision:   26.32%; recall:   25.00%; F1:   25.64; predicted:    19\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
            "\n",
            "-------------------- Test set quality: --------------------\n",
            "processed 13258 tokens with 604 phrases; found: 492 phrases; correct: 196.\n",
            "\n",
            "precision:  39.84%; recall:  32.45%; F1:  35.77\n",
            "\n",
            "\t     company: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:    22\n",
            "\n",
            "\t    facility: precision:   43.90%; recall:   38.30%; F1:   40.91; predicted:    41\n",
            "\n",
            "\t     geo-loc: precision:   67.67%; recall:   54.55%; F1:   60.40; predicted:   133\n",
            "\n",
            "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     5\n",
            "\n",
            "\t musicartist: precision:   11.76%; recall:    7.41%; F1:    9.09; predicted:    17\n",
            "\n",
            "\t       other: precision:   30.25%; recall:   34.95%; F1:   32.43; predicted:   119\n",
            "\n",
            "\t      person: precision:   39.78%; recall:   35.58%; F1:   37.56; predicted:    93\n",
            "\n",
            "\t     product: precision:   10.26%; recall:   14.29%; F1:   11.94; predicted:    39\n",
            "\n",
            "\t  sportsteam: precision:   47.37%; recall:   29.03%; F1:   36.00; predicted:    19\n",
            "\n",
            "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     4\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}